{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN use public data\n",
    "assume public data is KITTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "import dataloader\n",
    "import torchvision\n",
    "from utils import *\n",
    "from torch.nn import BCELoss\n",
    "from torch.autograd import grad\n",
    "import torchvision.utils as tvls\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from discri import DGWGAN\n",
    "from generator import Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freeze(net):\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad_(False) \n",
    "\n",
    "def unfreeze(net):\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "def gradient_penalty(x, y):\n",
    "    # interpolation\n",
    "    shape = [x.size(0)] + [1] * (x.dim() - 1)\n",
    "    alpha = torch.rand(shape).cuda()\n",
    "    z = x + alpha * (y - x)\n",
    "    z = z.cuda()\n",
    "    z.requires_grad = True\n",
    "\n",
    "    o = DG(z)\n",
    "    g = grad(o, z, grad_outputs = torch.ones(o.size()).cuda(), create_graph = True)[0].view(z.size(0), -1)\n",
    "    gp = ((g.norm(p = 2, dim = 1) - 1) ** 2).mean()\n",
    "\n",
    "    return gp\n",
    "\n",
    "save_img_dir = \"result/imgs_celeba_gan\"\n",
    "save_model_dir= \"result/models_celeba_gan\"\n",
    "os.makedirs(save_model_dir, exist_ok=True)\n",
    "os.makedirs(save_img_dir, exist_ok=True)\n",
    "\n",
    "dataset_name = \"celeba\"\n",
    "\n",
    "log_path = \"./attack_logs\"\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "log_file = \"GAN.txt\"\n",
    "utils.Tee(os.path.join(log_path, log_file), 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    file = \"./\" + dataset_name + \".json\"\n",
    "    args = load_params(json_file=file)\n",
    "\n",
    "    file_path = args['dataset']['train_file_path']\n",
    "    model_name = args['dataset']['model_name']\n",
    "    lr = args[model_name]['lr']\n",
    "    batch_size = args[model_name]['batch_size']\n",
    "    z_dim = args[model_name]['z_dim']\n",
    "    epochs = args[model_name]['epochs']\n",
    "    n_critic = args[model_name]['n_critic']\n",
    "\n",
    "    print(\"---------------------Training [%s]------------------------------\" % model_name)\n",
    "    utils.print_params(args[\"dataset\"], args[model_name])\n",
    "\n",
    "    dataset, dataloader = init_dataloader(args, file_path, batch_size, mode=\"gan\")\n",
    "\n",
    "    G = Generator(z_dim)\n",
    "    DG = DGWGAN(3)\n",
    "    \n",
    "    G = torch.nn.DataParallel(G).cuda()\n",
    "    DG = torch.nn.DataParallel(DG).cuda()\n",
    "\n",
    "    dg_optimizer = torch.optim.Adam(DG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    g_optimizer = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for i, imgs in enumerate(dataloader):\n",
    "            step += 1\n",
    "            imgs = imgs.cuda()\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            freeze(G)\n",
    "            unfreeze(DG)\n",
    "\n",
    "            z = torch.randn(bs, z_dim).cuda()\n",
    "            f_imgs = G(z)\n",
    "\n",
    "            r_logit = DG(imgs)\n",
    "            f_logit = DG(f_imgs)\n",
    "            \n",
    "            wd = r_logit.mean() - f_logit.mean()  # Wasserstein-1 Distance\n",
    "            gp = gradient_penalty(imgs.data, f_imgs.data)\n",
    "            dg_loss = - wd + gp * 10.0\n",
    "            \n",
    "            dg_optimizer.zero_grad()\n",
    "            dg_loss.backward()\n",
    "            dg_optimizer.step()\n",
    "\n",
    "            # train G\n",
    "\n",
    "            if step % n_critic == 0:\n",
    "                freeze(DG)\n",
    "                unfreeze(G)\n",
    "                z = torch.randn(bs, z_dim).cuda()\n",
    "                f_imgs = G(z)\n",
    "                logit_dg = DG(f_imgs)\n",
    "                # calculate g_loss\n",
    "                g_loss = - logit_dg.mean()\n",
    "                \n",
    "                g_optimizer.zero_grad()\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "        interval = end - start\n",
    "        print(\"Epoch:%d \\t Time:%.2f\" % (epoch, interval))\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            z = torch.randn(32, z_dim).cuda()\n",
    "            fake_image = G(z)\n",
    "            save_tensor_images(fake_image.detach(), os.path.join(save_img_dir, \"result_image_{}.png\".format(epoch)), nrow = 8)\n",
    "        \n",
    "        torch.save({'state_dict':G.state_dict()}, os.path.join(save_model_dir, \"celeba_G.tar\"))\n",
    "        torch.save({'state_dict':DG.state_dict()}, os.path.join(save_model_dir, \"celeba_D.tar\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7ff9c0dded1d52c3c1a9a8606639aad76ffdee4ff0b707021cc10d9bb7ae1b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
