{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from models import DeepLabV3\n",
    "from datasets.cityscapes import CityScapes\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import colorize_mask\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data\n",
    "\n",
    "\n",
    "root_dir = \"F:\\COMP90055\\GMIDA\\datas\\CityScapes\"\n",
    "dataloader = CityScapes(root_dir,batch_size=1,split='val',shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "weight = r\"F:\\COMP90055\\GMIDA\\train-runs\\2022-09-22_22-44-38\\checkpoint\\deeplabv3-0.pth\"\n",
    "state_dict = torch.load(weight, map_location=device)\n",
    "# print(state_dict)\n",
    "model = DeepLabV3().to(device)      \n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# output dir\n",
    "run_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\",time.localtime())\n",
    "output_dir = \"./output/seg\"\n",
    "output_dir = os.path.join(output_dir,run_time)\n",
    "if os.path.exists(output_dir):\n",
    "    os.makedir(output_dir)\n",
    "\n",
    "# visualize\n",
    "def visualize(im,gt,pred):\n",
    "    vis_transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    im = im[0].data.cpu()\n",
    "    im = np.array(im.permute(1,2,0))\n",
    "\n",
    "    im = vis_transform(im)\n",
    "    \n",
    "    gt = gt[0].data.cpu().numpy()\n",
    "    gt = colorize_mask(gt).convert('RGB')\n",
    "    gt = vis_transform(gt)\n",
    "\n",
    "    pred = pred[0].data.max(0)[1].cpu().numpy()\n",
    "\n",
    "    pred = colorize_mask(pred).convert('RGB')\n",
    "    plt.imshow(pred)\n",
    "    pred = vis_transform(pred)\n",
    "    \n",
    "    grid = torch.stack([im,gt,pred],0)\n",
    "    grid = make_grid(grid.cpu(), nrow=3, padding=5)\n",
    "    return grid\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    plt.figure(figsize=(32,16))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9dd558b350640609c4b77bff5d5113a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tkinter import image_names\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for index,image,label in tqdm(dataloader):\n",
    "        # print(image.size()) # B 3 1024 2048\n",
    "        # print(torch.min(image),torch.max(image)) # 0~255\n",
    "        \n",
    "        resize = T.Resize((128,256))\n",
    "        \n",
    "        image = resize(image)\n",
    "\n",
    "        # print(image.size()) # 4 3 128 256\n",
    "        # plt.imshow(im[0].data.permute(1,2,0).numpy())\n",
    "        # plt.show()\n",
    "        # print(torch.min(image),torch.max(image)) # 0~255\n",
    "        output = model((image*255).to(device))['out']\n",
    "        # print(index)\n",
    "        output = output[0].data.max(0)[1].cpu().numpy().astype(np.uint8)\n",
    "        output = Image.fromarray(output)\n",
    "        label = resize(label)[0].cpu().numpy().astype(np.uint8)\n",
    "        \n",
    "        label = Image.fromarray(label)\n",
    "        output.save(os.path.join(r\"F:\\COMP90055\\GMIDA\\results\\source_only\\cityscapes128\",index[0]+'.png'))\n",
    "        label.save(os.path.join(r\"F:\\COMP90055\\GMIDA\\ground_truth\\cityscapes128\",index[0]+'.png'))\n",
    "        # plt.imshow(label)\n",
    "        # # print(output.shape)\n",
    "        # # print(np.unique(output))\n",
    "        # # grid = visualize(image,label,output)\n",
    "\n",
    "        # show(grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
